Title,Authors,Abstract,PDF Link
Learningthe non-Markovian features of subsystem dynamics,"Michele Coppola,Mari Carmen Bañuls,Zala Lenarčič","The dynamics of local observables in a quantum many-body system can be formally described in the language of open systems. The problem is that the bath representing the complement of the local subsystem generally does not allow the common simplifications often crucial for such a framework. Leveraging tensor network calculations and optimization tools frommachinelearning, we extract and characterize the dynamical maps for single- and two-site subsystems embedded in an infinite quantum Ising chain after a global quench. We consider three paradigmatic regimes: integrable critical, integrable non-critical, and chaotic. For each we find the optimal time-local representation of the subsystem dynamics at different times. We explore the properties of thelearnedtime-dependent Liouvillians and whether they can be used to forecast the long-time dynamics of local observables beyond the times accessible through direct quantum many-body numerical simulation. Our procedure naturally suggests a novel measure of non-Markovianity based on the distance between the quasi-exact dynamical map and the closest CP-divisible form and reveals that criticality leads to the closest Markovian representation at large times.△ Less",https://arxiv.orghttps://arxiv.org/pdf/2507.14133
Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective,"Pankaj Yadav,Vivek Vijay","Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalancelearning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.△ Less",https://arxiv.orghttps://arxiv.org/pdf/2507.14121
NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining,"Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev","Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.△ Less",https://arxiv.orghttps://arxiv.org/pdf/2507.14119
Quantum BoltzmannMachinesusing Parallel Annealing for Medical Image Classification,"Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner","Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum BoltzmannMachines(QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of Noè et al. (2024), who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost 70 % compared to regular annealing-based BM executions.△ Less",https://arxiv.orghttps://arxiv.org/pdf/2507.14116
CUDA-L1: Improving CUDA Optimization via Contrastive ReinforcementLearning,"Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum","The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcementlearningframework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques andlearnsto combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcementlearningcan transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.△ Less",https://arxiv.orghttps://arxiv.org/pdf/2507.14111
